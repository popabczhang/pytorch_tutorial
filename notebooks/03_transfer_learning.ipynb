{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with PyTorch\n",
    "We're going to train a neural network to classify dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ppt import utils\n",
    "from ppt.utils import attr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# list(get_trainable(model.parameters()))\n",
    "# list(get_frozen(model.parameters()))\n",
    "# all_trainable(model.parameters())\n",
    "# all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data - DogsCatsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224  #224  #defined by NN model input\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)  # some images are pretty small\n",
    "    #transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)\n",
    "    #transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary\n",
    "#from ppt.utils import DogsCatsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data set\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"sample/valid\", transform=val_trans)\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# full data set\n",
    "# use ppt.utils\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"valid\", transform=val_trans)\n",
    "# use pytorch_version default\n",
    "train_ds = ImageFolder(\"../data/raw/DUI/train\", transform=train_trans, loader=default_loader)\n",
    "val_ds = ImageFolder(\"../data/raw/DUI/valid\", transform=train_trans, loader=default_loader)\n",
    "\n",
    "BATCH_SIZE = 220  #2  #256  #512  #32\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13787, 1421)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "img = to_pil(val_ds[4][0])\n",
    "\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Batch loading for datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "PyTorch offers quite a few [pre-trained networks](https://pytorch.org/docs/stable/torchvision/models.html) for you to use:\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "\n",
    "And there are more available via [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n",
    "- NASNet,\n",
    "- ResNeXt,\n",
    "- InceptionV4,\n",
    "- InceptionResnetV2, \n",
    "- Xception, \n",
    "- DPN,\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "#model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#model = models.resnet101(pretrained=True)\n",
    "model = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model.parameters())\n",
    "assert all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the last layer with a linear layer. New layers have `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(2048, n_classes)  # according to the model, 512 for resnet18, 2048 for resnet50 & resnet101 & resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetitive\n",
    "def get_model(n_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    freeze_all(model.parameters())\n",
    "    model.fc = nn.Linear(512, n_classes)\n",
    "    return model\n",
    "\n",
    "#model = get_model().to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),\n",
    "    # model.fc.parameters(),\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20  #1  #2  #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "    batch loss: 0.760\n",
      "    batch loss: 0.615\n",
      "    batch loss: 0.642\n",
      "    batch loss: 0.914\n",
      "    batch loss: 0.625\n",
      "    batch loss: 0.562\n",
      "    batch loss: 0.597\n",
      "    batch loss: 0.643\n",
      "    batch loss: 0.607\n",
      "    batch loss: 0.573\n",
      "    batch loss: 0.494\n",
      "    batch loss: 0.527\n",
      "    batch loss: 0.472\n",
      "    batch loss: 0.548\n",
      "    batch loss: 0.486\n",
      "    batch loss: 0.488\n",
      "    batch loss: 0.409\n",
      "    batch loss: 0.417\n",
      "    batch loss: 0.435\n",
      "    batch loss: 0.420\n",
      "    batch loss: 0.436\n",
      "    batch loss: 0.393\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.357\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.369\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.387\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.357\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.357\n",
      "    batch loss: 0.389\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.346\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.346\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.316\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.313\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.352\n",
      "    batch loss: 0.379\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.359\n",
      "  Train Loss: 0.4107847797037978\n",
      "  Train Acc:  0.8066294335243345\n",
      "  Valid Loss: 0.3318282609712398\n",
      "  Valid Acc:  0.8423645320197044\n",
      "\n",
      "Epoch 2/20\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.335\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.335\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.344\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.361\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.316\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.398\n",
      "    batch loss: 0.288\n",
      "  Train Loss: 0.29993249623054535\n",
      "  Train Acc:  0.869659824472329\n",
      "  Valid Loss: 0.27842773248577185\n",
      "  Valid Acc:  0.8831808585503167\n",
      "\n",
      "Epoch 3/20\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.289\n",
      "  Train Loss: 0.2763067357836014\n",
      "  Train Acc:  0.8837310509900631\n",
      "  Valid Loss: 0.26481040553003027\n",
      "  Valid Acc:  0.8979591836734694\n",
      "\n",
      "Epoch 4/20\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.412\n",
      "    batch loss: 0.359\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.314\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.313\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.291\n",
      "  Train Loss: 0.2669303936557807\n",
      "  Train Acc:  0.888590701385363\n",
      "  Valid Loss: 0.26872758046159606\n",
      "  Valid Acc:  0.8937368050668544\n",
      "\n",
      "Epoch 5/20\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.364\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.314\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.184\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.196\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.279\n",
      "  Train Loss: 0.2627744188014389\n",
      "  Train Acc:  0.8919271777761659\n",
      "  Valid Loss: 0.25952362661878464\n",
      "  Valid Acc:  0.9007741027445461\n",
      "\n",
      "Epoch 6/20\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.188\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.237\n",
      "  Train Loss: 0.2546559266587705\n",
      "  Train Acc:  0.8953361862624212\n",
      "  Valid Loss: 0.2505936329210417\n",
      "  Valid Acc:  0.8993666432090077\n",
      "\n",
      "Epoch 7/20\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.157\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.191\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.311\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.266\n",
      "  Train Loss: 0.2537859450951959\n",
      "  Train Acc:  0.8960615072169434\n",
      "  Valid Loss: 0.26061408860342844\n",
      "  Valid Acc:  0.8951442646023927\n",
      "\n",
      "Epoch 8/20\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.187\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.192\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.385\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.335\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.211\n",
      "  Train Loss: 0.255745947288578\n",
      "  Train Acc:  0.8930876913034017\n",
      "  Valid Loss: 0.2478657681953731\n",
      "  Valid Acc:  0.9021815622800844\n",
      "\n",
      "Epoch 9/20\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.415\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.393\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.210\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.205\n",
      "  Train Loss: 0.2523017060393963\n",
      "  Train Acc:  0.8950460578806122\n",
      "  Valid Loss: 0.24859290625074226\n",
      "  Valid Acc:  0.8993666432090077\n",
      "\n",
      "Epoch 10/20\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.171\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.155\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.335\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.316\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.218\n",
      "  Train Loss: 0.25045988131418545\n",
      "  Train Acc:  0.8968593602669181\n",
      "  Valid Loss: 0.27764970426992325\n",
      "  Valid Acc:  0.8859957776213934\n",
      "\n",
      "Epoch 11/20\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.179\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.189\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.189\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.194\n",
      "    batch loss: 0.194\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.186\n",
      "  Train Loss: 0.24635368640059496\n",
      "  Train Acc:  0.8973670849350838\n",
      "  Valid Loss: 0.24353955565498883\n",
      "  Valid Acc:  0.9035890218156228\n",
      "\n",
      "Epoch 12/20\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.163\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.177\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.142\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.155\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.363\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.210\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.262\n",
      "  Train Loss: 0.2405336703855201\n",
      "  Train Acc:  0.9016464785667658\n",
      "  Valid Loss: 0.24238046139667438\n",
      "  Valid Acc:  0.904292751583392\n",
      "\n",
      "Epoch 13/20\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.180\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.195\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.183\n",
      "  Train Loss: 0.2480951566396393\n",
      "  Train Acc:  0.8958439109305868\n",
      "  Valid Loss: 0.24963255007446525\n",
      "  Valid Acc:  0.904292751583392\n",
      "\n",
      "Epoch 14/20\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.188\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.192\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.195\n",
      "    batch loss: 0.185\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.187\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.188\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.252\n",
      "  Train Loss: 0.2375043726338342\n",
      "  Train Acc:  0.9039675056212374\n",
      "  Valid Loss: 0.24397466431865383\n",
      "  Valid Acc:  0.9028852920478536\n",
      "\n",
      "Epoch 15/20\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.179\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.154\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.198\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.181\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.178\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.158\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.187\n",
      "    batch loss: 0.181\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.191\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.239\n",
      "  Train Loss: 0.2347193395649625\n",
      "  Train Acc:  0.9048378907666642\n",
      "  Valid Loss: 0.23675571102059115\n",
      "  Valid Acc:  0.9085151301900071\n",
      "\n",
      "Epoch 16/20\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.330\n",
      "    batch loss: 0.173\n",
      "    batch loss: 0.198\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.158\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.198\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.282\n",
      "  Train Loss: 0.2450297699583669\n",
      "  Train Acc:  0.8976572133168927\n",
      "  Valid Loss: 0.2397657643733605\n",
      "  Valid Acc:  0.9064039408866995\n",
      "\n",
      "Epoch 17/20\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.171\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.184\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.180\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.178\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.177\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.182\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.176\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.181\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.210\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.224\n",
      "  Train Loss: 0.2329186705110849\n",
      "  Train Acc:  0.9061434684848045\n",
      "  Valid Loss: 0.23340337906481093\n",
      "  Valid Acc:  0.904292751583392\n",
      "\n",
      "Epoch 18/20\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.183\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.195\n",
      "    batch loss: 0.190\n",
      "    batch loss: 0.191\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.180\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.346\n",
      "    batch loss: 0.160\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.170\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.198\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.176\n",
      "    batch loss: 0.198\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.210\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.132\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.262\n",
      "  Train Loss: 0.23440886786541393\n",
      "  Train Acc:  0.9028069920940016\n",
      "  Valid Loss: 0.2318434974704302\n",
      "  Valid Acc:  0.9134412385643913\n",
      "\n",
      "Epoch 19/20\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.171\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.167\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.195\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.157\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.313\n",
      "    batch loss: 0.205\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.168\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.181\n",
      "    batch loss: 0.192\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.325\n",
      "  Train Loss: 0.23143292915192476\n",
      "  Train Acc:  0.9044026981939508\n",
      "  Valid Loss: 0.231511376719505\n",
      "  Valid Acc:  0.912033779028853\n",
      "\n",
      "Epoch 20/20\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.381\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.165\n",
      "    batch loss: 0.188\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.194\n",
      "    batch loss: 0.189\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.181\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.203\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.178\n",
      "    batch loss: 0.379\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.215\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.201\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.195\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.192\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.204\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.199\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.174\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.219\n",
      "  Train Loss: 0.23394157837630788\n",
      "  Train Acc:  0.9054181475302822\n",
      "  Valid Loss: 0.2337790477410074\n",
      "  Valid Acc:  0.9071076706544687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        correct += (y_label_ == y).sum().item()\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "    print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    model.eval()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "                    \n",
    "            y_ = model(X)\n",
    "        \n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            \n",
    "            loss = criterion(y_, y)\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "    print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** -- above is the result of crop extent data set -- **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermission: training libraries\n",
    "\n",
    "Writing the training loop is my least favourite thing about PyTorch.\n",
    "\n",
    "Keras is great here!\n",
    "```python\n",
    "model.compile(optimizer, criterion, metrics=[\"accuracy\", \"f1\"])\n",
    "model.fit(X, y, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "### [Ignite](https://github.com/pytorch/ignite)\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "\n",
    "### [TNT](https://github.com/pytorch/tnt)\n",
    "> TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "> [...]\n",
    "> The project was inspired by TorchNet, and legend says that it stood for “TorchNetTwo”\n",
    "\n",
    "\n",
    "### [Skorch](https://github.com/dnouri/skorch)\n",
    "> A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "\n",
    "### \"The fun of Reinvention\"\n",
    "Clearly, there must be a better way! Write your own lib (but don't use it) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ignite.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b1361f655698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mignite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from ignite.metrics import (\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mCategoricalAccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mPrecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ignite.metrics'"
     ]
    }
   ],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    },\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}] Batch[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "# trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "\n",
    "Demo: https://github.com/lanpa/tensorboard-pytorch/blob/master/screenshots/Demo.gif\n",
    "\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r tf_log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_{random.randint(0, 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new SummaryWriter for new experiment\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.0001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\"accuracy\": CategoricalAccuracy(), \"loss\": Loss(criterion)},\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])\n",
    "\n",
    "\n",
    "trainer.run(train_dl, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visdom\n",
    "https://github.com/facebookresearch/visdom\n",
    "![](https://camo.githubusercontent.com/d69475a01f9f327fc42931a21df8134d1fbdfc19/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d62714839555843772d42452f574c3255736472726241492f41414141414141416e59632f656d727877436d6e7257345f434c54797955747442305359524a2d693443436951434c63422f73302f53637265656e2b53686f742b323031372d30332d30362b61742b31302e35312e30322b414d2e706e67253232766973646f6d5f626967253232)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
