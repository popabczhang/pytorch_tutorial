{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with PyTorch\n",
    "We're going to train a neural network to classify dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ppt import utils\n",
    "from ppt.utils import attr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# list(get_trainable(model.parameters()))\n",
    "# list(get_frozen(model.parameters()))\n",
    "# all_trainable(model.parameters())\n",
    "# all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data - DogsCatsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224  #224  #defined by NN model input\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)  # some images are pretty small\n",
    "    #transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)\n",
    "    #transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary\n",
    "#from ppt.utils import DogsCatsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data set\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"sample/valid\", transform=val_trans)\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# full data set\n",
    "# use ppt.utils\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"valid\", transform=val_trans)\n",
    "# use pytorch_version default\n",
    "train_ds = ImageFolder(\"../data/raw/DUI/train\", transform=train_trans, loader=default_loader)\n",
    "val_ds = ImageFolder(\"../data/raw/DUI/valid\", transform=train_trans, loader=default_loader)\n",
    "\n",
    "BATCH_SIZE = 128  #2  #256  #512  #32\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13787, 1421)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "img = to_pil(val_ds[4][0])\n",
    "\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Batch loading for datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "PyTorch offers quite a few [pre-trained networks](https://pytorch.org/docs/stable/torchvision/models.html) for you to use:\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "\n",
    "And there are more available via [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n",
    "- NASNet,\n",
    "- ResNeXt,\n",
    "- InceptionV4,\n",
    "- InceptionResnetV2, \n",
    "- Xception, \n",
    "- DPN,\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "#model = models.resnet18(pretrained=True)\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model.parameters())\n",
    "assert all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the last layer with a linear layer. New layers have `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(2048, n_classes)  # according to the model, 512 for resnet18, 2048 for resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetitive\n",
    "def get_model(n_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    freeze_all(model.parameters())\n",
    "    model.fc = nn.Linear(512, n_classes)\n",
    "    return model\n",
    "\n",
    "#model = get_model().to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),\n",
    "    # model.fc.parameters(),\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5  #1  #2  #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    batch loss: 0.809\n",
      "    batch loss: 0.648\n",
      "    batch loss: 0.709\n",
      "    batch loss: 0.744\n",
      "    batch loss: 0.587\n",
      "    batch loss: 0.566\n",
      "    batch loss: 0.594\n",
      "    batch loss: 0.591\n",
      "    batch loss: 0.617\n",
      "    batch loss: 0.578\n",
      "    batch loss: 0.554\n",
      "    batch loss: 0.507\n",
      "    batch loss: 0.715\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.536\n",
      "    batch loss: 0.473\n",
      "    batch loss: 0.537\n",
      "    batch loss: 0.584\n",
      "    batch loss: 0.501\n",
      "    batch loss: 0.542\n",
      "    batch loss: 0.603\n",
      "    batch loss: 0.500\n",
      "    batch loss: 0.548\n",
      "    batch loss: 0.537\n",
      "    batch loss: 0.548\n",
      "    batch loss: 0.499\n",
      "    batch loss: 0.587\n",
      "    batch loss: 0.524\n",
      "    batch loss: 0.561\n",
      "    batch loss: 0.514\n",
      "    batch loss: 0.563\n",
      "    batch loss: 0.509\n",
      "    batch loss: 0.461\n",
      "    batch loss: 0.492\n",
      "    batch loss: 0.518\n",
      "    batch loss: 0.438\n",
      "    batch loss: 0.451\n",
      "    batch loss: 0.485\n",
      "    batch loss: 0.432\n",
      "    batch loss: 0.531\n",
      "    batch loss: 0.455\n",
      "    batch loss: 0.511\n",
      "    batch loss: 0.490\n",
      "    batch loss: 0.471\n",
      "    batch loss: 0.492\n",
      "    batch loss: 0.456\n",
      "    batch loss: 0.420\n",
      "    batch loss: 0.524\n",
      "    batch loss: 0.521\n",
      "    batch loss: 0.531\n",
      "    batch loss: 0.454\n",
      "    batch loss: 0.475\n",
      "    batch loss: 0.452\n",
      "    batch loss: 0.434\n",
      "    batch loss: 0.476\n",
      "    batch loss: 0.445\n",
      "    batch loss: 0.534\n",
      "    batch loss: 0.459\n",
      "    batch loss: 0.445\n",
      "    batch loss: 0.499\n",
      "    batch loss: 0.453\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.472\n",
      "    batch loss: 0.463\n",
      "    batch loss: 0.392\n",
      "    batch loss: 0.436\n",
      "    batch loss: 0.471\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.398\n",
      "    batch loss: 0.473\n",
      "    batch loss: 0.512\n",
      "    batch loss: 0.418\n",
      "    batch loss: 0.472\n",
      "    batch loss: 0.438\n",
      "    batch loss: 0.399\n",
      "    batch loss: 0.446\n",
      "    batch loss: 0.482\n",
      "    batch loss: 0.461\n",
      "    batch loss: 0.387\n",
      "    batch loss: 0.385\n",
      "    batch loss: 0.425\n",
      "    batch loss: 0.484\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.405\n",
      "    batch loss: 0.496\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.476\n",
      "    batch loss: 0.485\n",
      "    batch loss: 0.392\n",
      "    batch loss: 0.475\n",
      "    batch loss: 0.504\n",
      "    batch loss: 0.434\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.426\n",
      "    batch loss: 0.430\n",
      "    batch loss: 0.440\n",
      "    batch loss: 0.449\n",
      "    batch loss: 0.412\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.434\n",
      "    batch loss: 0.408\n",
      "    batch loss: 0.412\n",
      "    batch loss: 0.447\n",
      "    batch loss: 0.431\n",
      "    batch loss: 0.386\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.405\n",
      "    batch loss: 0.408\n",
      "  Train Loss: 0.48611740282130583\n",
      "  Train Acc:  0.7575252049031697\n",
      "  Valid Loss: 0.4281649272430119\n",
      "  Valid Acc:  0.7783251231527094\n",
      "\n",
      "Epoch 2/5\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.387\n",
      "    batch loss: 0.384\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.469\n",
      "    batch loss: 0.348\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.435\n",
      "    batch loss: 0.482\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.407\n",
      "    batch loss: 0.433\n",
      "    batch loss: 0.402\n",
      "    batch loss: 0.444\n",
      "    batch loss: 0.381\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.415\n",
      "    batch loss: 0.460\n",
      "    batch loss: 0.385\n",
      "    batch loss: 0.408\n",
      "    batch loss: 0.416\n",
      "    batch loss: 0.371\n",
      "    batch loss: 0.452\n",
      "    batch loss: 0.410\n",
      "    batch loss: 0.454\n",
      "    batch loss: 0.400\n",
      "    batch loss: 0.431\n",
      "    batch loss: 0.437\n",
      "    batch loss: 0.431\n",
      "    batch loss: 0.402\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.359\n",
      "    batch loss: 0.453\n",
      "    batch loss: 0.417\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.355\n",
      "    batch loss: 0.431\n",
      "    batch loss: 0.359\n",
      "    batch loss: 0.388\n",
      "    batch loss: 0.441\n",
      "    batch loss: 0.410\n",
      "    batch loss: 0.442\n",
      "    batch loss: 0.389\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.379\n",
      "    batch loss: 0.360\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.471\n",
      "    batch loss: 0.489\n",
      "    batch loss: 0.419\n",
      "    batch loss: 0.424\n",
      "    batch loss: 0.432\n",
      "    batch loss: 0.487\n",
      "    batch loss: 0.447\n",
      "    batch loss: 0.363\n",
      "    batch loss: 0.455\n",
      "    batch loss: 0.385\n",
      "    batch loss: 0.456\n",
      "    batch loss: 0.442\n",
      "    batch loss: 0.429\n",
      "    batch loss: 0.369\n",
      "    batch loss: 0.386\n",
      "    batch loss: 0.429\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.370\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.396\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.399\n",
      "    batch loss: 0.373\n",
      "    batch loss: 0.503\n",
      "    batch loss: 0.365\n",
      "    batch loss: 0.355\n",
      "    batch loss: 0.414\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.395\n",
      "    batch loss: 0.341\n",
      "    batch loss: 0.474\n",
      "    batch loss: 0.427\n",
      "    batch loss: 0.464\n",
      "    batch loss: 0.445\n",
      "    batch loss: 0.427\n",
      "    batch loss: 0.436\n",
      "    batch loss: 0.363\n",
      "    batch loss: 0.417\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.341\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.326\n",
      "    batch loss: 0.425\n",
      "    batch loss: 0.342\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.491\n",
      "    batch loss: 0.407\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.392\n",
      "    batch loss: 0.335\n",
      "  Train Loss: 0.4032317573449035\n",
      "  Train Acc:  0.8062667730470733\n",
      "  Valid Loss: 0.38578441325918544\n",
      "  Valid Acc:  0.8191414496833216\n",
      "\n",
      "Epoch 3/5\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.446\n",
      "    batch loss: 0.352\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.348\n",
      "    batch loss: 0.330\n",
      "    batch loss: 0.416\n",
      "    batch loss: 0.358\n",
      "    batch loss: 0.425\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.390\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.371\n",
      "    batch loss: 0.437\n",
      "    batch loss: 0.447\n",
      "    batch loss: 0.413\n",
      "    batch loss: 0.422\n",
      "    batch loss: 0.460\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.470\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.365\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.347\n",
      "    batch loss: 0.390\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.360\n",
      "    batch loss: 0.537\n",
      "    batch loss: 0.417\n",
      "    batch loss: 0.384\n",
      "    batch loss: 0.410\n",
      "    batch loss: 0.381\n",
      "    batch loss: 0.382\n",
      "    batch loss: 0.424\n",
      "    batch loss: 0.509\n",
      "    batch loss: 0.400\n",
      "    batch loss: 0.453\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.400\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.375\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.355\n",
      "    batch loss: 0.368\n",
      "    batch loss: 0.346\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.377\n",
      "    batch loss: 0.328\n",
      "    batch loss: 0.373\n",
      "    batch loss: 0.407\n",
      "    batch loss: 0.377\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.497\n",
      "    batch loss: 0.413\n",
      "    batch loss: 0.429\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.373\n",
      "    batch loss: 0.456\n",
      "    batch loss: 0.368\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.396\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.439\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.401\n",
      "    batch loss: 0.428\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.343\n",
      "    batch loss: 0.362\n",
      "    batch loss: 0.393\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.356\n",
      "    batch loss: 0.417\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.330\n",
      "    batch loss: 0.403\n",
      "    batch loss: 0.447\n",
      "    batch loss: 0.454\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.408\n",
      "    batch loss: 0.359\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.431\n",
      "    batch loss: 0.327\n",
      "    batch loss: 0.390\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.408\n",
      "    batch loss: 0.430\n",
      "    batch loss: 0.342\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.377\n",
      "    batch loss: 0.363\n",
      "    batch loss: 0.416\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.333\n",
      "  Train Loss: 0.3793223063538693\n",
      "  Train Acc:  0.8193225502284761\n",
      "  Valid Loss: 0.3788665342632941\n",
      "  Valid Acc:  0.8318085855031668\n",
      "\n",
      "Epoch 4/5\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.393\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.448\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.362\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.396\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.334\n",
      "    batch loss: 0.449\n",
      "    batch loss: 0.400\n",
      "    batch loss: 0.387\n",
      "    batch loss: 0.404\n",
      "    batch loss: 0.364\n",
      "    batch loss: 0.356\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.466\n",
      "    batch loss: 0.330\n",
      "    batch loss: 0.424\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.382\n",
      "    batch loss: 0.399\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.423\n",
      "    batch loss: 0.415\n",
      "    batch loss: 0.316\n",
      "    batch loss: 0.436\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.347\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.356\n",
      "    batch loss: 0.389\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.357\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.399\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.358\n",
      "    batch loss: 0.422\n",
      "    batch loss: 0.369\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.465\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.360\n",
      "    batch loss: 0.362\n",
      "    batch loss: 0.369\n",
      "    batch loss: 0.424\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.433\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.407\n",
      "    batch loss: 0.349\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.414\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.430\n",
      "    batch loss: 0.482\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.419\n",
      "    batch loss: 0.466\n",
      "    batch loss: 0.451\n",
      "    batch loss: 0.369\n",
      "    batch loss: 0.392\n",
      "    batch loss: 0.465\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.361\n",
      "    batch loss: 0.392\n",
      "    batch loss: 0.390\n",
      "    batch loss: 0.460\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.463\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.429\n",
      "    batch loss: 0.342\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.383\n",
      "    batch loss: 0.368\n",
      "    batch loss: 0.370\n",
      "    batch loss: 0.426\n",
      "    batch loss: 0.364\n",
      "  Train Loss: 0.37063616423201523\n",
      "  Train Acc:  0.825995503010082\n",
      "  Valid Loss: 0.37687477467346997\n",
      "  Valid Acc:  0.8388458831808585\n",
      "\n",
      "Epoch 5/5\n",
      "    batch loss: 0.423\n",
      "    batch loss: 0.454\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.349\n",
      "    batch loss: 0.388\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.377\n",
      "    batch loss: 0.423\n",
      "    batch loss: 0.364\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.355\n",
      "    batch loss: 0.418\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.385\n",
      "    batch loss: 0.437\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.362\n",
      "    batch loss: 0.371\n",
      "    batch loss: 0.351\n",
      "    batch loss: 0.357\n",
      "    batch loss: 0.537\n",
      "    batch loss: 0.401\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.311\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.356\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.374\n",
      "    batch loss: 0.325\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.508\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.361\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.376\n",
      "    batch loss: 0.414\n",
      "    batch loss: 0.396\n",
      "    batch loss: 0.402\n",
      "    batch loss: 0.386\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.397\n",
      "    batch loss: 0.442\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.401\n",
      "    batch loss: 0.352\n",
      "    batch loss: 0.365\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.364\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.367\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.352\n",
      "    batch loss: 0.469\n",
      "    batch loss: 0.430\n",
      "    batch loss: 0.419\n",
      "    batch loss: 0.411\n",
      "    batch loss: 0.377\n",
      "    batch loss: 0.511\n",
      "    batch loss: 0.432\n",
      "    batch loss: 0.378\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.542\n",
      "    batch loss: 0.358\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.348\n",
      "    batch loss: 0.372\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.390\n",
      "    batch loss: 0.350\n",
      "    batch loss: 0.334\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.379\n",
      "    batch loss: 0.420\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.326\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.420\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.333\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.409\n",
      "    batch loss: 0.391\n",
      "    batch loss: 0.347\n",
      "    batch loss: 0.360\n",
      "    batch loss: 0.335\n",
      "  Train Loss: 0.3630687834613515\n",
      "  Train Acc:  0.8328860520780446\n",
      "  Valid Loss: 0.3837568977615684\n",
      "  Valid Acc:  0.8346235045742435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        correct += (y_label_ == y).sum().item()\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "    print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    model.eval()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "                    \n",
    "            y_ = model(X)\n",
    "        \n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            \n",
    "            loss = criterion(y_, y)\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "    print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermission: training libraries\n",
    "\n",
    "Writing the training loop is my least favourite thing about PyTorch.\n",
    "\n",
    "Keras is great here!\n",
    "```python\n",
    "model.compile(optimizer, criterion, metrics=[\"accuracy\", \"f1\"])\n",
    "model.fit(X, y, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "### [Ignite](https://github.com/pytorch/ignite)\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "\n",
    "### [TNT](https://github.com/pytorch/tnt)\n",
    "> TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "> [...]\n",
    "> The project was inspired by TorchNet, and legend says that it stood for “TorchNetTwo”\n",
    "\n",
    "\n",
    "### [Skorch](https://github.com/dnouri/skorch)\n",
    "> A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "\n",
    "### \"The fun of Reinvention\"\n",
    "Clearly, there must be a better way! Write your own lib (but don't use it) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ignite.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b1361f655698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mignite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from ignite.metrics import (\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mCategoricalAccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mPrecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ignite.metrics'"
     ]
    }
   ],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    },\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}] Batch[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "# trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "\n",
    "Demo: https://github.com/lanpa/tensorboard-pytorch/blob/master/screenshots/Demo.gif\n",
    "\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r tf_log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_{random.randint(0, 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new SummaryWriter for new experiment\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.0001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\"accuracy\": CategoricalAccuracy(), \"loss\": Loss(criterion)},\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])\n",
    "\n",
    "\n",
    "trainer.run(train_dl, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visdom\n",
    "https://github.com/facebookresearch/visdom\n",
    "![](https://camo.githubusercontent.com/d69475a01f9f327fc42931a21df8134d1fbdfc19/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d62714839555843772d42452f574c3255736472726241492f41414141414141416e59632f656d727877436d6e7257345f434c54797955747442305359524a2d693443436951434c63422f73302f53637265656e2b53686f742b323031372d30332d30362b61742b31302e35312e30322b414d2e706e67253232766973646f6d5f626967253232)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
