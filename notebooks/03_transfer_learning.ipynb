{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with PyTorch\n",
    "We're going to train a neural network to classify dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ppt import utils\n",
    "from ppt.utils import attr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# list(get_trainable(model.parameters()))\n",
    "# list(get_frozen(model.parameters()))\n",
    "# all_trainable(model.parameters())\n",
    "# all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data - DogsCatsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224  #224  #defined by NN model input\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)  # some images are pretty small\n",
    "    #transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)\n",
    "    #transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary\n",
    "#from ppt.utils import DogsCatsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data set\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"sample/valid\", transform=val_trans)\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# full data set\n",
    "# use ppt.utils\n",
    "#train_ds = DogsCatsDataset(\"../data/raw\", \"train\", transform=train_trans)\n",
    "#val_ds = DogsCatsDataset(\"../data/raw\", \"valid\", transform=val_trans)\n",
    "# use pytorch_version default\n",
    "train_ds = ImageFolder(\"../data/raw/DUI/train\", transform=train_trans, loader=default_loader)\n",
    "val_ds = ImageFolder(\"../data/raw/DUI/valid\", transform=train_trans, loader=default_loader)\n",
    "\n",
    "BATCH_SIZE = 400  #2  #256  #512  #32  #220 for resnet152 on Dell Presison 5520 laptop, 500 for resnet18\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13787, 1421)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "img = to_pil(val_ds[4][0])\n",
    "\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Batch loading for datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "PyTorch offers quite a few [pre-trained networks](https://pytorch.org/docs/stable/torchvision/models.html) for you to use:\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "\n",
    "And there are more available via [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n",
    "- NASNet,\n",
    "- ResNeXt,\n",
    "- InceptionV4,\n",
    "- InceptionResnetV2, \n",
    "- Xception, \n",
    "- DPN,\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#model = models.resnet101(pretrained=True)\n",
    "#model = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model.parameters())\n",
    "assert all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the last layer with a linear layer. New layers have `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, n_classes)  # according to the model, 512 for resnet18, 2048 for resnet50 & resnet101 & resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# repetitive\n",
    "def get_model(n_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    freeze_all(model.parameters())\n",
    "    model.fc = nn.Linear(512, n_classes)\n",
    "    return model'''\n",
    "\n",
    "#model = get_model().to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),\n",
    "    # model.fc.parameters(),\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20  #1  #2  #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "    batch loss: 0.654\n",
      "    batch loss: 0.544\n",
      "    batch loss: 0.557\n",
      "    batch loss: 0.621\n",
      "    batch loss: 0.548\n",
      "    batch loss: 0.499\n",
      "    batch loss: 0.512\n",
      "    batch loss: 0.479\n",
      "    batch loss: 0.480\n",
      "    batch loss: 0.487\n",
      "    batch loss: 0.474\n",
      "    batch loss: 0.473\n",
      "    batch loss: 0.411\n",
      "    batch loss: 0.485\n",
      "    batch loss: 0.425\n",
      "    batch loss: 0.422\n",
      "    batch loss: 0.429\n",
      "    batch loss: 0.444\n",
      "    batch loss: 0.352\n",
      "    batch loss: 0.423\n",
      "    batch loss: 0.394\n",
      "    batch loss: 0.381\n",
      "    batch loss: 0.365\n",
      "    batch loss: 0.386\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.328\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.406\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.371\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.343\n",
      "    batch loss: 0.400\n",
      "  Train Loss: 0.43201318897899743\n",
      "  Train Acc:  0.7917603539566258\n",
      "  Valid Loss: 0.3338113287483446\n",
      "  Valid Acc:  0.8550316678395496\n",
      "\n",
      "Epoch 2/20\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.368\n",
      "    batch loss: 0.340\n",
      "    batch loss: 0.350\n",
      "    batch loss: 0.358\n",
      "    batch loss: 0.354\n",
      "    batch loss: 0.366\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.313\n",
      "    batch loss: 0.314\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.342\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.331\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.336\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.353\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.365\n",
      "    batch loss: 0.358\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.286\n",
      "  Train Loss: 0.32351199843655765\n",
      "  Train Acc:  0.8550808732864292\n",
      "  Valid Loss: 0.31142647550778857\n",
      "  Valid Acc:  0.8669950738916257\n",
      "\n",
      "Epoch 3/20\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.316\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.328\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.359\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.314\n",
      "    batch loss: 0.248\n",
      "  Train Loss: 0.30237868214445585\n",
      "  Train Acc:  0.8677739899905709\n",
      "  Valid Loss: 0.2896487524943986\n",
      "  Valid Acc:  0.8817733990147784\n",
      "\n",
      "Epoch 4/20\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.323\n",
      "    batch loss: 0.317\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.338\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.321\n",
      "    batch loss: 0.318\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.339\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.327\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.291\n",
      "  Train Loss: 0.29215420280096366\n",
      "  Train Acc:  0.872706172481323\n",
      "  Valid Loss: 0.28002172825958926\n",
      "  Valid Acc:  0.8916256157635468\n",
      "\n",
      "Epoch 5/20\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.319\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.322\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.302\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.330\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.272\n",
      "  Train Loss: 0.2823157156945624\n",
      "  Train Acc:  0.880322042503808\n",
      "  Valid Loss: 0.2737945439864178\n",
      "  Valid Acc:  0.8944405348346235\n",
      "\n",
      "Epoch 6/20\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.300\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.324\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.328\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.249\n",
      "  Train Loss: 0.27617231059270675\n",
      "  Train Acc:  0.8851816928991079\n",
      "  Valid Loss: 0.26830608881669177\n",
      "  Valid Acc:  0.8923293455313159\n",
      "\n",
      "Epoch 7/20\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.210\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.332\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.290\n",
      "  Train Loss: 0.27152060483618595\n",
      "  Train Acc:  0.8850366287082034\n",
      "  Valid Loss: 0.2776509843016912\n",
      "  Valid Acc:  0.8859957776213934\n",
      "\n",
      "Epoch 8/20\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.291\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.320\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.274\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.299\n",
      "  Train Loss: 0.2688086555375401\n",
      "  Train Acc:  0.887067527380866\n",
      "  Valid Loss: 0.26239433446289534\n",
      "  Valid Acc:  0.8979591836734694\n",
      "\n",
      "Epoch 9/20\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.220\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.231\n",
      "  Train Loss: 0.2655082454160099\n",
      "  Train Acc:  0.8892434902444332\n",
      "  Valid Loss: 0.26118965852772325\n",
      "  Valid Acc:  0.8958479943701618\n",
      "\n",
      "Epoch 10/20\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.287\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.337\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.207\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.298\n",
      "  Train Loss: 0.26145347306084543\n",
      "  Train Acc:  0.8917821135852615\n",
      "  Valid Loss: 0.2548253814323447\n",
      "  Valid Acc:  0.900070372976777\n",
      "\n",
      "Epoch 11/20\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.345\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.329\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.295\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.212\n",
      "  Train Loss: 0.2592913367153035\n",
      "  Train Acc:  0.8932327554943063\n",
      "  Valid Loss: 0.25613531168290715\n",
      "  Valid Acc:  0.8916256157635468\n",
      "\n",
      "Epoch 12/20\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.223\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.305\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.309\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.233\n",
      "  Train Loss: 0.2601229166722137\n",
      "  Train Acc:  0.8918546456807137\n",
      "  Valid Loss: 0.2513448566009594\n",
      "  Valid Acc:  0.8986629134412386\n",
      "\n",
      "Epoch 13/20\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.307\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.293\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.243\n",
      "  Train Loss: 0.2556392229596846\n",
      "  Train Acc:  0.8935954159715674\n",
      "  Valid Loss: 0.25025293417668193\n",
      "  Valid Acc:  0.9064039408866995\n",
      "\n",
      "Epoch 14/20\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.285\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.290\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.284\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.234\n",
      "    batch loss: 0.276\n",
      "  Train Loss: 0.25550481781678\n",
      "  Train Acc:  0.893160223398854\n",
      "  Valid Loss: 0.25000685963053504\n",
      "  Valid Acc:  0.9035890218156228\n",
      "\n",
      "Epoch 15/20\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.303\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.297\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.212\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.256\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.217\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.206\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.286\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.257\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.275\n",
      "  Train Loss: 0.25414333663772204\n",
      "  Train Acc:  0.8947559294988032\n",
      "  Valid Loss: 0.2629431717669268\n",
      "  Valid Acc:  0.88881069669247\n",
      "\n",
      "Epoch 16/20\n",
      "    batch loss: 0.299\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.306\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.289\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.312\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.264\n",
      "  Train Loss: 0.25222220904263193\n",
      "  Train Acc:  0.8929426271124973\n",
      "  Valid Loss: 0.24915647586410436\n",
      "  Valid Acc:  0.8993666432090077\n",
      "\n",
      "Epoch 17/20\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.266\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.208\n",
      "    batch loss: 0.292\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.276\n",
      "    batch loss: 0.261\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.271\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.315\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.211\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.233\n",
      "  Train Loss: 0.2523879073925012\n",
      "  Train Acc:  0.8960615072169434\n",
      "  Valid Loss: 0.25126661497635544\n",
      "  Valid Acc:  0.9014778325123153\n",
      "\n",
      "Epoch 18/20\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.240\n",
      "    batch loss: 0.298\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.231\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.294\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.254\n",
      "    batch loss: 0.250\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.245\n",
      "    batch loss: 0.264\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.242\n",
      "  Train Loss: 0.25209693830505436\n",
      "  Train Acc:  0.8954087183578734\n",
      "  Valid Loss: 0.2534276372441769\n",
      "  Valid Acc:  0.9028852920478536\n",
      "\n",
      "Epoch 19/20\n",
      "    batch loss: 0.228\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.237\n",
      "    batch loss: 0.233\n",
      "    batch loss: 0.247\n",
      "    batch loss: 0.269\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.248\n",
      "    batch loss: 0.243\n",
      "    batch loss: 0.267\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.251\n",
      "    batch loss: 0.262\n",
      "    batch loss: 0.301\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.176\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.239\n",
      "    batch loss: 0.268\n",
      "    batch loss: 0.258\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.265\n",
      "    batch loss: 0.232\n",
      "    batch loss: 0.282\n",
      "    batch loss: 0.213\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.283\n",
      "    batch loss: 0.279\n",
      "    batch loss: 0.246\n",
      "    batch loss: 0.274\n",
      "  Train Loss: 0.24977322461996412\n",
      "  Train Acc:  0.8958439109305868\n",
      "  Valid Loss: 0.2577117656335958\n",
      "  Valid Acc:  0.8944405348346235\n",
      "\n",
      "Epoch 20/20\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.259\n",
      "    batch loss: 0.275\n",
      "    batch loss: 0.244\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.249\n",
      "    batch loss: 0.225\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.230\n",
      "    batch loss: 0.227\n",
      "    batch loss: 0.277\n",
      "    batch loss: 0.241\n",
      "    batch loss: 0.270\n",
      "    batch loss: 0.272\n",
      "    batch loss: 0.296\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.226\n",
      "    batch loss: 0.218\n",
      "    batch loss: 0.221\n",
      "    batch loss: 0.219\n",
      "    batch loss: 0.304\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.242\n",
      "    batch loss: 0.224\n",
      "    batch loss: 0.278\n",
      "    batch loss: 0.253\n",
      "    batch loss: 0.280\n",
      "    batch loss: 0.288\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.255\n",
      "    batch loss: 0.254\n",
      "  Train Loss: 0.2523111120766688\n",
      "  Train Acc:  0.8954812504533256\n",
      "  Valid Loss: 0.24986562332076812\n",
      "  Valid Acc:  0.8972554539057002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        correct += (y_label_ == y).sum().item()\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "    print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    model.eval()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "                    \n",
    "            y_ = model(X)\n",
    "        \n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            \n",
    "            loss = criterion(y_, y)\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "    print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** -- above is the result of crop extent data set -- **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermission: training libraries\n",
    "\n",
    "Writing the training loop is my least favourite thing about PyTorch.\n",
    "\n",
    "Keras is great here!\n",
    "```python\n",
    "model.compile(optimizer, criterion, metrics=[\"accuracy\", \"f1\"])\n",
    "model.fit(X, y, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "### [Ignite](https://github.com/pytorch/ignite)\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "\n",
    "### [TNT](https://github.com/pytorch/tnt)\n",
    "> TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "> [...]\n",
    "> The project was inspired by TorchNet, and legend says that it stood for “TorchNetTwo”\n",
    "\n",
    "\n",
    "### [Skorch](https://github.com/dnouri/skorch)\n",
    "> A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "\n",
    "### \"The fun of Reinvention\"\n",
    "Clearly, there must be a better way! Write your own lib (but don't use it) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ignite.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b1361f655698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mignite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from ignite.metrics import (\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mCategoricalAccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mPrecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ignite.metrics'"
     ]
    }
   ],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    },\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}] Batch[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "# trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "\n",
    "Demo: https://github.com/lanpa/tensorboard-pytorch/blob/master/screenshots/Demo.gif\n",
    "\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r tf_log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_{random.randint(0, 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new SummaryWriter for new experiment\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.0001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\"accuracy\": CategoricalAccuracy(), \"loss\": Loss(criterion)},\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])\n",
    "\n",
    "\n",
    "trainer.run(train_dl, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visdom\n",
    "https://github.com/facebookresearch/visdom\n",
    "![](https://camo.githubusercontent.com/d69475a01f9f327fc42931a21df8134d1fbdfc19/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d62714839555843772d42452f574c3255736472726241492f41414141414141416e59632f656d727877436d6e7257345f434c54797955747442305359524a2d693443436951434c63422f73302f53637265656e2b53686f742b323031372d30332d30362b61742b31302e35312e30322b414d2e706e67253232766973646f6d5f626967253232)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
